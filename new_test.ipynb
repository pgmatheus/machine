{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras import layers\n",
    "from keras.layers import ReLU, Input, LeakyReLU, Conv2D, Activation, MaxPool2D, Flatten, Dense, Reshape, Conv2DTranspose, UpSampling2D, BatchNormalization, Dropout\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, Model\n",
    "\"\"\" from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img \"\"\"\n",
    "\"\"\" from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave \"\"\"\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\"\"\" from PIL import Image, ImageFile \"\"\"\n",
    "\n",
    "length = 1\n",
    "\n",
    "print(tf. __version__)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, start_from_epoch=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1002 images belonging to 1 classes.\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "path = 'G:\\\\Deletar\\\\pictures_conv\\\\new_tr' \n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train = train_datagen.flow_from_directory(path, target_size=(256, 256),batch_size=length,class_mode=None)\n",
    "\n",
    "X =[]\n",
    "Y =[]\n",
    "for img in train[0]:\n",
    "  try:\n",
    "      lab = rgb2lab(img)\n",
    "      X.append(lab[:,:,0])\n",
    "      Y.append(lab[:,:,1:] / 128)\n",
    "  except:\n",
    "     print('error')\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X = X.reshape(X.shape+(1,))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39mencoder_input, outputs\u001b[39m=\u001b[39mdecoder_output)\n\u001b[0;32m     22\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m , metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m model\u001b[39m.\u001b[39;49mfit(X,Y,validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1686\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1683\u001b[0m split_at \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mfloor(batch_dim \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m validation_split)))\n\u001b[0;32m   1685\u001b[0m \u001b[39mif\u001b[39;00m split_at \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m split_at \u001b[39m==\u001b[39m batch_dim:\n\u001b[1;32m-> 1686\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1687\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining data contains \u001b[39m\u001b[39m{batch_dim}\u001b[39;00m\u001b[39m samples, which is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1688\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msufficient to split it into a validation and training set as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1689\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified by `validation_split=\u001b[39m\u001b[39m{validation_split}\u001b[39;00m\u001b[39m`. Either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1690\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide more data, or a different value for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1691\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` argument.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1692\u001b[0m             batch_dim\u001b[39m=\u001b[39mbatch_dim, validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[0;32m   1693\u001b[0m         )\n\u001b[0;32m   1694\u001b[0m     )\n\u001b[0;32m   1696\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split\u001b[39m(t, start, end):\n\u001b[0;32m   1697\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])\n",
    "model.fit(X,Y,validation_split=0.2, epochs=10 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24686d21cc7785692d55d98c2e2e4af933b78d648d008261ee4e205553102dea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
